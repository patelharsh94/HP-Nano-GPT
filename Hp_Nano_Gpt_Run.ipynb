{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNBg7BgS//OwsuRBv5amJa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patelharsh94/HP-Nano-GPT/blob/main/Hp_Nano_Gpt_Run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ySv32DmR-iQ",
        "outputId": "ae8c391c-41f0-43aa-fa9b-31cefd03fd55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 65\n",
            "Training data size: 1003850\n",
            "Validation data size: 111539\n",
            "First 100 characters of training data: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
            "Input batch shape: torch.Size([64, 256])\n",
            "Target batch shape: torch.Size([64, 256])\n",
            "10.79M parameters\n",
            "step 0: train loss 4.3359, val loss 4.3320\n",
            "step 500: train loss 2.0108, val loss 2.0921\n",
            "step 1000: train loss 1.6133, val loss 1.7899\n",
            "step 1500: train loss 1.4430, val loss 1.6547\n",
            "step 2000: train loss 1.3514, val loss 1.5741\n",
            "step 2500: train loss 1.2834, val loss 1.5315\n",
            "step 3000: train loss 1.2352, val loss 1.5109\n",
            "step 3500: train loss 1.1897, val loss 1.4818\n",
            "step 4000: train loss 1.1492, val loss 1.4841\n",
            "step 4500: train loss 1.1144, val loss 1.4799\n",
            "step 4999: train loss 1.0765, val loss 1.4861\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import hyperparameters as hp\n",
        "from BigramLanguageModel import BigramLanguageModel\n",
        "\n",
        "# Getting the tiny shakespeare training data.\n",
        "# Read the dataset\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# create the vocabulary mapping\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encode the text into integers\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "# Decode the integers back into text\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# convert the text into a list of integers\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# get the train and test data\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Training data size: {len(train_data)}\")\n",
        "print(f\"Validation data size: {len(val_data)}\")\n",
        "print(f\"First 100 characters of training data: {train_data[:100]}\")\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    \"\"\"\n",
        "    Generate a batch of data.\n",
        "    split: 'train' or 'val'\n",
        "    \"\"\"\n",
        "    batch_data = train_data if split == 'train' else val_data\n",
        "    # Randomly sample batch_size sequences of length block_size\n",
        "    ix = torch.randint(len(batch_data) - hp.block_size, (hp.batch_size,))\n",
        "    # x is the input data, y is the target data\n",
        "    x = torch.stack([batch_data[i:i + hp.block_size] for i in ix])\n",
        "    y = torch.stack([batch_data[i + 1:i + hp.block_size + 1] for i in ix])\n",
        "    x, y = x.to(hp.device), y.to(hp.device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "xbatch, ybatch = get_batch('train')\n",
        "print(f\"Input batch shape: {xbatch.shape}\")\n",
        "print(f\"Target batch shape: {ybatch.shape}\")\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(hp.device)\n",
        "\n",
        "print(f\"{sum(p.numel() for p in m.parameters())/1e6:.2f}M parameters\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=hp.learning_rate)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    # Helper function to estimate loss on train/val sets\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(hp.eval_iters)\n",
        "        for k in range(hp.eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# The training loop\n",
        "for iter in range(hp.max_iters):\n",
        "    if iter % hp.eval_interval == 0 or iter == hp.max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text from the trained model\n",
        "print(\"\\n--- Generating Text ---\")\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=hp.device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=1000)[0].tolist())\n",
        "print(generated_chars)\n",
        "print(\"--------------------\")"
      ],
      "metadata": {
        "id": "j0MajLDUlJTF",
        "outputId": "32cd6ffe-1bbd-4f9f-bae1-90a3775fe509",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Text ---\n",
            "\n",
            "I am sure so troops to me why.\n",
            "\n",
            "AEdieu;\n",
            "Heard me.\n",
            "\n",
            "Citizen:\n",
            "Fear heir, my lord, and could, not.\n",
            "\n",
            "Second Murderer:\n",
            "And leave him with interrior live holds!\n",
            "\n",
            "CLARENCE:\n",
            "Heaven me with me speak, Warwick; for I sweet, I do;\n",
            "And was with already is have made a greation.\n",
            "\n",
            "FLORIZEL:\n",
            "So she seems both my lord.\n",
            "\n",
            "First Murderer:\n",
            "Madam,' adauda!\n",
            "\n",
            "\n",
            "CLARENCE:\n",
            "Second murderer:\n",
            "Art thou seam out on the chance of privymence,\n",
            "tarquing pitches to make me incourse to your, and see you me\n",
            "To may not upon your purpose receives!\n",
            "But, wrongs are I to the house of Norfelthumberland,\n",
            "And, that lovest sight you lenfects, Ethreugh for sense\n",
            "Braft Bishop and Sicilia?--it must this first.\n",
            "\n",
            "YORK:\n",
            "Take please your change: my fair queen.\n",
            "You bid! I leave you him, good Duke of York;\n",
            "To buy the rew thereaf carried I'll but,\n",
            "Yet you lie in clear-gyfeit prace, son your Englanc\n",
            "And gentle muster for my poor gross,\n",
            "Vith proud Noble branches fair blood of your will.\n",
            "\n",
            "GREY:\n",
            "With madne, there's that rages him a vile wrong,\n",
            "Tha\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWZgXeH3lH2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}